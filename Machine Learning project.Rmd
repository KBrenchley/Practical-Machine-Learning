---
output: html_document
---
## Machine Learning - Predicting Dumbbells

### Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

This paper discusses building a model that predicts the manner in which the health participants did the exercise, and discusses the use of cross validation and the expected out-of-sample error.


```{r,echo=FALSE}
setwd("~/Documents/Coursera/Practical Machine Learning/Project/Practical-Machine-Learning")

##download.file(url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile="pml-training.csv",method="curl")
##download.file(url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",destfil="pml-testing.csv",method = "curl")

training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")

library(caret)
library(randomForest)
library(tree)
library(rpart)
library(gbm)
library(survival)
library(splines)
library(plyr)
set.seed(11854)

```

### Exploring the Data

Inspecting an initial summary of the training data showed a number of variables containing mostly NA values, and others containing very similar values, as shown by the very narrow range and skewness of these boxplots with some of the variables associated with the belt sensors.


```{r,echo=FALSE,fig.height = 6, fig.width = 8, fig.align='center'}

## -------- Data examination ---------

## charts and plots
t <- training
belts <- data.frame(t$roll_belt, t$pitch_belt, t$yaw_belt, t$total_accel_belt,
                    t$max_roll_belt, t$max_picth_belt, t$max_yaw_belt)

par(mfrow = c(1, 2))
boxplot(belts, main="Some Belt Variables")

boxplot(t$yaw_belt ~ t$classe, main= "Yaw Belt with Classe", xlab="Classe", ylab="Yaw Belt")
par(mfrow = c(1, 1))
```
In addition, out of the `r nrow(training)` rows in the training set, many of the variables, such as var_accel_arm, min_roll_belt, and min_pitch_dumbbell had merely `r sum(!is.na(t$min_roll_belt))` values, with the rest NA. This was a small enough number that imputing the remaining `r nrow(training) - sum(!is.na(t$min_roll_belt))` values would likely not add enough variability to be meaningful.

I chose to begin selecting variables for my model by cleaning out the similar and mostly NA variables. I reduced the training set from 160 variables to 100 variables by removing those with near zero variance. I reduced the training set further down to 59 variables by removing those with NA values.

```{r,echo=FALSE}

## so let's remove zero covariates (variables whose values are very similar)

near.zero <- nearZeroVar(training,saveMetrics = TRUE)
training.new <- training[,near.zero$nzv == FALSE]

## and get rid of the ones that are mostly NA

training.NA <- training.new[colSums(is.na(training.new)) > 0]
training.new <- training.new[colSums(is.na(training.new)) == 0]
```

### Starting Cross Validation

Even with the reduced set I still had many variables. I began to build a cross validation set. To do this, I used the K-fold method to create 10 training sets and 10 test sets out of the training set. 

```{r, echo=FALSE}

## ---------- Training Sets ----------

## make new training sets based on training.new

train.folds <- createFolds(y = training.new$classe, k=10, list=TRUE, returnTrain=TRUE)
test.folds <- createFolds(y = training.new$classe, k=10, list=TRUE, returnTrain=FALSE)
## sapply(train.folds, length)
## sapply(test.folds, length)

train.set1 <- train.folds[[1]]
train1 <- training.new[train.set1,]
test.set1 <- test.folds[[1]]
test1 <- training[test.set1,]

## fix K-fold testing set so it can be predicted on (remove answers)
len <- length(test1$classe)
test1$classe <- NULL
test1$problem_id <- 1:len

```

This gave me 10 training sets with length of around `r length(train1$classe)` and 10 testing sets (also taken from the training set) with length of around `r length(test1$problem_id)`. I also needed to remove the classe variable from these test sets so that prediction could happen.

### Beginning to Build a Model

I began to build a model on the new training set. To do this, I began by examining the correlated predictors. The sensors were focused on the belt, arm, forearm, and dumbbell of each participant, with several sensors measuring different values, so I suspected that there might be strong correlation between many of these variables. Using the cor() function, I identified 24 variables in the remaining set with greater than .80 correlation, and with classe created a small training set. Out of curiosity, I also created a smaller training set with greater than .90 correlation. (Note that by "small" and "smaller" I mean fewer predictors for the model.) I began trying these two versions of the training set with the different algorithms.


```{r, echo=FALSE}

## ===============================================
## correlated predictors and principal compoenents

## converting factors, char variables to numbers
train1$cvtd_timestamp <-  as.numeric(train1$cvtd_timestamp)
train1$user_name <- as.numeric(train1$user_name)
test1$cvtd_timestamp <-  as.numeric(test1$cvtd_timestamp)
test1$user_name <- as.numeric(test1$user_name)

## Finding the variables that have useful variation
M <- abs(cor(train1[,-59]))
diag(M) <- 0
cor.pred <- which(M > 0.8, arr.ind=T)

train1.small <- train1[,c(2,5,7,8,9,10,14,15,16,17,24,25,27,30,31,32,34,35,37,39,40,42,51,52)]
train1.small$classe <- train1$classe

## now with > 90%
cor.pred.90 <- which(M > 0.9, arr.ind=T)

train1.smaller <- train1[,c(7,8,10,14,15,16,24,25,37,39,52)]
train1.smaller$classe <- train1$classe

```


The variables for **the small training set** are: `r names(train1.small)`.

The variables for **the smaller training set** are: `r names(train1.smaller)`.

Since the response variable is a factor rather than continuous, linear models would be difficult to fit for this data set. To that end, I decided to use the first of my 10 training sets with different tree functions, and with boost, to compare the models' accuracy levels. I chose to compare accuracy of the rpart, tree, randomForest, and boost functions.

### Testing Models with Cross-Validation Training Set 1

Here are some of the results from my experiments with fitting models using different functions. Not all are shown due to space limitations.


```{r,echo=FALSE}

## ===========================================================
## Try models using the new training sets

## tracking accuracy

methods <- vector()
small <- vector()
smaller <- vector()

## ---rpart----
fitRpart.small <- train(classe ~ ., data = train1.small, method="rpart")
fitRpart.smaller <- train(classe ~ ., data = train1.smaller, method="rpart")

#print(fitRpart.small$finalModel)
plot(fitRpart.small$finalModel, uniform=TRUE, main="rpart Tree with Small Training Set")
text(fitRpart.small$finalModel, use.n=TRUE, all=TRUE, cex=.8)

##print(fitRpart.smaller$finalModel)
plot(fitRpart.smaller$finalModel, uniform=TRUE, main="rpart Tree with Smaller Training Set")
text(fitRpart.smaller$finalModel, use.n=TRUE, all=TRUE, cex=.8)

predictionRpart.small <- predict(fitRpart.small,test1)
predictionRpart.smaller <- predict(fitRpart.smaller,test1)

## Test prediction accuracy
methods <- c(methods, "rpart")
temp <- training[test.set1,]

resultsRpart.small <- data.frame(predictionRpart.small, temp$classe)
accuracyRpart.small <- sum(resultsRpart.small[,1] == 
                                 resultsRpart.small[,2])/length(temp$classe)
small <- c(small, accuracyRpart.small)

resultsRpart.smaller <- data.frame(predictionRpart.smaller, temp$classe)
accuracyRpart.smaller <- sum(resultsRpart.smaller[,1] == 
                                 resultsRpart.smaller[,2])/length(temp$classe)
smaller <- c(smaller, accuracyRpart.smaller)

## ----Tree----
fitTree.small <- tree(classe ~ ., data = train1.small)
fitTree.smaller <- tree(classe ~ ., data = train1.smaller)

##fitTree.small
plot(fitTree.small, main = "Tree with Small Training Set")
text(fitTree.small)

predictionTree.small <- predict(fitTree.small, test1)
predictionTree.smaller <- predict(fitTree.smaller, test1)

## look at variable importance
##round(importance(predictionTree.small), 2)
##round(importance(predictionTree.smaller), 2)

## Test prediction accuracy
methods <- c(methods, "tree")
temp <- training[test.set1,]

resultsTree.small <- data.frame(predictionTree.small, temp$classe)
accTree.small <- sum(resultsTree.small[,1] == resultsTree.small[,2])/length(temp$classe)
small <- c(small,accTree.small)

resultsTree.smaller <- data.frame(predictionTree.smaller, temp$classe)
accTree.smaller <- sum(resultsTree.smaller[,1] == resultsTree.smaller[,2])/length(temp$classe)
smaller <- c(smaller,accTree.smaller)

## ----Random Forests----

## with train -- way too long

## with RandomForest

fitRf.small <- randomForest(classe ~ ., data = train1.small, importance=TRUE, proximity = TRUE)
fitRf.smaller <- randomForest(classe ~ ., data = train1.smaller, importance=TRUE, proximity = TRUE)

predictionRf.small <- predict(fitRf.small, test1)
predictionRf.smaller <- predict(fitRf.smaller, test1)

## Test prediction accuracy
methods <- c(methods, "randomForest")
temp <- training[test.set1,]

resultsRf.small <- data.frame(predictionRf.small, temp$classe)
accRf.small <-sum(resultsRf.small[,1] == resultsRf.small[,2])/length(temp$classe)
small <- c(small,accRf.small)

resultsRf.smaller <- data.frame(predictionRf.smaller, temp$classe)
accRf.smaller <-sum(resultsRf.smaller[,1] == resultsRf.smaller[,2])/length(temp$classe)
smaller <- c(smaller,accRf.smaller)

## ----Boosting ----
## Using gbm (boosting with trees)

fitBoost.small <- train(classe ~ ., data = train1.small, method="gbm", verbose=FALSE)
fitBoost.smaller <- train(classe ~ ., data = train1.smaller, method="gbm", verbose=FALSE)

predictionBoost.small <- predict(fitBoost.small, test1)
predictionBoost.smaller <- predict(fitBoost.smaller, test1)

## Test prediction accuracy
methods <- c(methods,"boost")
temp <- training[test.set1,]

resultsBoost.small <- data.frame(predictionBoost.small, temp$classe)
accBoost.small <-sum(resultsBoost.small[,1] == resultsBoost.small[,2])/length(temp$classe)
small <- c(small, accBoost.small)

resultsBoost.smaller <- data.frame(predictionRf.smaller, temp$classe)
accBoost.smaller <-sum(resultsBoost.smaller[,1] == resultsBoost.smaller[,2])/length(temp$classe)
smaller <- c(smaller, accBoost.smaller)

## Accuracy comparison

accuracy.all <- data.frame(methods, small, smaller)

```

### Accuracy of Tests

```{r}
accuracy.all
```

Upon examining the accuracy of the different functions, and compared with the different sizes of training sets, the **randomForest** function with the small training set had the highest accuracy of the eight attempts, though boost was close, and both randomForest and boost, using the smaller training set, weren't far behind. It's interesting to see that having more predictors improved the accuracy rate for some, though not all, of the functions. The randomForest function, with the small data set, appears to be the best choice.

### Cross Validation with Chosen Model

Now that I had built a model on the training set, and tested it on the test set, I needed to repeat the evaluation using the chosen model (randomForest) with all of the cross validation training and test sets, and average the estimated errors. For each of the 10 training/test sets, I measured the accuracy of the prediction by counting the number of correct predictions, divided by the number of observations in the test set.


```{r, echo=FALSE}
## -----------validate model-------------

## Use cross validation to make sure that randomForest is best choice

cv.accuracy <- vector()

## use small training set variables
training.cv <- training.new[,c(2,5,7,8,9,10,14,15,16,17,24,25,27,30,31,32,34,35,37,39,40,42,51,52)]
training.cv$classe <- training.new$classe

## Cross Validation: Make training/testing sets out of training set
k <- 10
traincv.folds <- createFolds(y = training.cv$classe, k=10, list=TRUE, returnTrain=TRUE)
testcv.folds <- createFolds(y = training.cv$classe, k=10, list=TRUE, returnTrain=FALSE)
##sapply(traincv.folds, length)
##sapply(testcv.folds, length)

for (i in 1:k) {
    traincv.set <- traincv.folds[[i]]
    train.temp <- training.cv[traincv.set,]
    testcv.set <- testcv.folds[[i]]
    test.temp <- training[testcv.set,]
    test.temp.real <- test.temp
    
    len <- length(test.temp$classe)
    test.temp$classe <- NULL
    test.temp$problem_id <- 1:len
    
    fit <- randomForest(classe ~ ., data = train.temp, importance=TRUE, 
                        proximity = TRUE)
    predictCV <- predict(fit, test.temp)

    ## Test prediction accuracy
    resultsCV <- data.frame(predictCV, test.temp.real$classe)
    accCV <-sum(resultsCV[,1] == resultsCV[,2])/length(test.temp.real$classe)
  
    cv.accuracy <- c(cv.accuracy, accCV)
}

```

After completing the cross validation tests to validate the model, the accuracy values for the 10 predictions were:

```{r,echo=FALSE}
round(cv.accuracy, 3)

```

The mean was `r round(mean(cv.accuracy), 3)`. With the accuracy that high, I chose to fit the model with the entire training and testing sets, and use this to predict the classe variable from the testing set.


```{r, echo=FALSE}
## ------------fit the final model, and predict the test set results---------

## set the training set to match our model

training.final <- training.new ## with zero covariates removed
testing.final <- testing

training.final$cvtd_timestamp <-  as.numeric(training.final$cvtd_timestamp)
training.final$user_name <- as.numeric(training.final$user_name)
testing.final$cvtd_timestamp <-  as.numeric(testing.final$cvtd_timestamp)
testing.final$user_name <- as.numeric(testing.final$user_name)

M <- abs(cor(training.final[,-59]))
diag(M) <- 0
cor.pred <- which(M > 0.8, arr.ind=T)

training.final <- training.final[,c(2,5,7,8,9,10,14,15,16,17,24,25,27,30,31,32,34,35,37,39,40,42,51,52)]
training.final$classe <- training$classe

fit.final <- randomForest(classe ~ ., data = training.final, importance=TRUE, 
                        proximity = TRUE)

predict.final <- predict(fit.final, testing.final)

```

### Conclusion

The final model is 

```{r,echo=FALSE}
print(fit.final)
## note OOB estimate of error rate is 0.67%
```

It's interesting to note in the final model that the OOB  estimate of the error rate is significantly lower than the cross validation accuracy results, which were 0.99. Still, the accuracy using the training and testing set with my randomForest model fit was 18/20, or `r round(18/20,2)`, slightly below the estimated out of sample error that was expected to have success be closer to 0.99. 

### References

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3P78EmOe4

